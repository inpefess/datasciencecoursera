{"name":"WeightLiftingExercise","tagline":"","body":"# Weight Lifting Exercise\r\nBoris Shminke  \r\n21.03.2015  \r\n\r\n## Introduction\r\n\r\nFor introduction about the data and complete research see [this site](http://groupware.les.inf.puc-rio.br/har).\r\nI used [this web-page](http://topepo.github.io/caret/training.html) for `caret` examples and GBM as a model because Owen Zhang [recommends](http://www.slideshare.net/freshdatabos/zhang-winning-datasciencecompetitions) it for simple modelling when you do not know what better to do instead.\r\n\r\n## Data acqusition and partitioning\r\n\r\nFirst, I defined libraries used, loaded data and set seed. For model tuning purposes I will use 2-fold cross-validation - an extremely fast method of assessing out of sample error. 2-fold cross validation is practically the same as using 50/50 training and testing subsets.\r\n\r\n\r\n```r\r\noptions(warn=-1)\r\nlibrary(caret)\r\ntrellis.par.set(caretTheme())\r\ntraining <- read.csv(\"pml-training.csv\")\r\nset.seed(17823)\r\nfitControl <- trainControl(\r\n  method = \"repeatedcv\",\r\n  number = 2,\r\n  repeats = 1,\r\n  verbose = FALSE)\r\n```\r\n\r\n## Data cleansing and feature selection\r\n\r\nI used `View` command in RStudio to preview overall data quality. After that I cleansed data a bit. Some variables have too many NAs (more than in a half of observations) and some are actually record IDs (name, timestamp, window flags). So I just dropped them and did not use for prediction.\r\n\r\n\r\n```r\r\ntraining <- training[ , c(8:11,37:49,60:68,84:86,102,113:124,140,151:160)]\r\n```\r\n\r\nSince we have only 53 predictors it is useless to use any compression technique like PCA.\r\n\r\n## Tuning number of trees\r\n\r\nFirst I tried 1000 trees, no interaction and 0.1 shrinkage.\r\n\r\n\r\n```r\r\ngbmGrid <- expand.grid(interaction.depth = 1,\r\n  n.trees = (1:20)*50,\r\n  shrinkage = 0.1)\r\n\r\ngbmFit1 <- train(classe ~ ., data = training,\r\n  method = \"gbm\",\r\n  trControl = fitControl,\r\n  tuneGrid = gbmGrid,\r\n  verbose = FALSE)\r\n```\r\n\r\n```\r\n## Loading required package: gbm\r\n## Loading required package: survival\r\n## \r\n## Attaching package: 'survival'\r\n## \r\n## The following object is masked from 'package:caret':\r\n## \r\n##     cluster\r\n## \r\n## Loading required package: splines\r\n## Loading required package: parallel\r\n## Loaded gbm 2.1.1\r\n## Loading required package: plyr\r\n```\r\n\r\n```r\r\nplot(gbmFit1)\r\n```\r\n\r\n![](https://raw.githubusercontent.com/inpefess/datasciencecoursera/master/PracticalMachineLearning/WeightLiftingExercise_files/figure-html/unnamed-chunk-3-1.png) \r\n\r\nThis means that 90% accuracy is achieved with only 300 boosting iterations and that next 700 make little impact.\r\n\r\n## Shrinkage tuning\r\n\r\nNow we will try to pick optimal shrinkage from `0.05, 0.1, ..., 0.95, 1` list for 300 trees and no interaction.\r\n\r\n\r\n```r\r\ngbmGrid <- expand.grid(shrinkage = c(1:20) * 0.05,\r\n  n.trees = (1:30)*10,\r\n  interaction.depth = 1)\r\n\r\ngbmFit2 <- train(classe ~ ., data = training,\r\n  method = \"gbm\",\r\n  trControl = fitControl,\r\n  tuneGrid = gbmGrid,\r\n  verbose = FALSE)\r\n\r\nplot(gbmFit2)\r\n```\r\n\r\n![](https://raw.githubusercontent.com/inpefess/datasciencecoursera/master/PracticalMachineLearning/WeightLiftingExercise_files/figure-html/unnamed-chunk-4-1.png) \r\n\r\nWe can see that large shrinkage is better but setting shrinkages more than `0.5` really gains nothing.\r\n\r\n## Interaction depth tuning\r\n\r\nOf course, the more interaction depth, the better, but computations become more time consuming. So the next experiment is about picking the trade off for interaction depth from list `1:10`, 300 trees and 0.5 shrinkage.\r\n\r\n\r\n```r\r\ngbmGrid <- expand.grid(n.trees = (1:30)*10,\r\n  shrinkage = 0.5,\r\n  interaction.depth = c(1:10))\r\n\r\ngbmFit3 <- train(classe ~ ., data = training,\r\n  method = \"gbm\",\r\n  trControl = fitControl,\r\n  tuneGrid = gbmGrid,\r\n  verbose = FALSE)\r\n```\r\n\r\n```r\r\nplot(gbmFit3)\r\n```\r\n\r\n![](https://raw.githubusercontent.com/inpefess/datasciencecoursera/master/PracticalMachineLearning/WeightLiftingExercise_files/figure-html/unnamed-chunk-5-1.png) \r\n\r\nWe can see that including interactions deeper than 5 is not very practical and also that number of trees can be decremented to 200 without substantial accuracy loss.\r\n\r\n## Final estimations for out of sample error\r\n\r\nTo summarise, we have chosen a GBM model with 200 trees, 0.5 shrinkage and interactions of depth 5. Out accuracy estimate is about 99%.\r\nFor more precise estimation we will use standard 10-fold cross-validation technique.\r\n\r\n\r\n```r\r\nfitControl <- trainControl(\r\n  method = \"repeatedcv\",\r\n  number = 10,\r\n  repeats = 1,\r\n  verbose = FALSE)\r\n\r\ngbmGrid <- expand.grid(n.trees = (1:20)*10,\r\n  shrinkage = 0.5,\r\n  interaction.depth = 5)\r\n\r\ngbmFit4 <- train(classe ~ ., data = training,\r\n  method = \"gbm\",\r\n  trControl = fitControl,\r\n  tuneGrid = gbmGrid,\r\n  verbose = FALSE)\r\n\r\nplot(gbmFit4)\r\n```\r\n\r\n![](https://raw.githubusercontent.com/inpefess/datasciencecoursera/master/PracticalMachineLearning/WeightLiftingExercise_files/figure-html/unnamed-chunk-6-1.png) \r\n\r\nOur model can be supposed to be highly accurate, about 99.5%\r\n\r\nAnother way of testing model is uploading a test set for evaluation. Let us generate required files.\r\n\r\n\r\n```r\r\ntesting <- read.csv(\"pml-testing.csv\")\r\nanswers <- predict(gbmFit4, testing)\r\n \r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(\"problem_id_\",i,\".txt\")\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\n \r\npml_write_files(answers)\r\n```\r\n\r\nAfter manual submission to the course site we eventually faced 100% accuracy of prediction for our 20 test observations. This is another piece of evidence of the built model quality.\r\n\r\n## Conclusion\r\n\r\nWe have built Gradient Boosting Model for 200 decision trees with interactions of depth 5 and shrinkage 0.5 using this variable list:\r\n\r\n\r\n```r\r\nnames(training)\r\n```\r\n\r\n```\r\n##  [1] \"roll_belt\"            \"pitch_belt\"           \"yaw_belt\"            \r\n##  [4] \"total_accel_belt\"     \"gyros_belt_x\"         \"gyros_belt_y\"        \r\n##  [7] \"gyros_belt_z\"         \"accel_belt_x\"         \"accel_belt_y\"        \r\n## [10] \"accel_belt_z\"         \"magnet_belt_x\"        \"magnet_belt_y\"       \r\n## [13] \"magnet_belt_z\"        \"roll_arm\"             \"pitch_arm\"           \r\n## [16] \"yaw_arm\"              \"total_accel_arm\"      \"gyros_arm_x\"         \r\n## [19] \"gyros_arm_y\"          \"gyros_arm_z\"          \"accel_arm_x\"         \r\n## [22] \"accel_arm_y\"          \"accel_arm_z\"          \"magnet_arm_x\"        \r\n## [25] \"magnet_arm_y\"         \"magnet_arm_z\"         \"roll_dumbbell\"       \r\n## [28] \"pitch_dumbbell\"       \"yaw_dumbbell\"         \"total_accel_dumbbell\"\r\n## [31] \"gyros_dumbbell_x\"     \"gyros_dumbbell_y\"     \"gyros_dumbbell_z\"    \r\n## [34] \"accel_dumbbell_x\"     \"accel_dumbbell_y\"     \"accel_dumbbell_z\"    \r\n## [37] \"magnet_dumbbell_x\"    \"magnet_dumbbell_y\"    \"magnet_dumbbell_z\"   \r\n## [40] \"roll_forearm\"         \"pitch_forearm\"        \"yaw_forearm\"         \r\n## [43] \"total_accel_forearm\"  \"gyros_forearm_x\"      \"gyros_forearm_y\"     \r\n## [46] \"gyros_forearm_z\"      \"accel_forearm_x\"      \"accel_forearm_y\"     \r\n## [49] \"accel_forearm_z\"      \"magnet_forearm_x\"     \"magnet_forearm_y\"    \r\n## [52] \"magnet_forearm_z\"     \"classe\"\r\n```\r\n\r\nThe model has high predicting power (ca. 99.5% accuracy) according to, first, 10-fold cross-validation performed on the training set and, second, scoring of independent test set from 20 observations.","google":"UA-60569668-1","note":"Don't delete this file! It's used internally to help with page regeneration."}